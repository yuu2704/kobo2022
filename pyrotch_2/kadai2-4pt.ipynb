{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【第２回目　課題4】 PyTorchを使った深層学習 (4): ImageNet 学習済モデルの利用．DCNN特徴抽出とファインチューニング．\n",
    "\n",
    "PyTorchでは，<a href=\"https://pytorch.org/docs/stable/torchvision/models.html\">TORCHVISION MODELS</a> を使うことで，ImageNetの学習済モデルが簡単に利用できます．\n",
    "\n",
    "### <a href=\"https://pytorch.org/docs/stable/torchvision/models.html\">学習済モデル自動読み込み</a>\n",
    "torchvision.datasets と同様に，自動ダウンロード機能を備えた torchvision.modelsのモジュール群が用意されています．\n",
    "* AlexNet\n",
    "* VGG（定番のVGG．パラメータが512MBもあって巨大なのが難点．)\n",
    "* ResNet\n",
    "* SqueezeNet\n",
    "* DenseNet\n",
    "* Inception v3\n",
    "* GoogLeNet\n",
    "* ShuffleNet v2\n",
    "* MobileNet v2 (Googleのモバイル用ネットワーク．パラメータが10MB未満．）\n",
    "* ResNeXt\n",
    "* Wide ResNet\n",
    "* MNASNet (最新の自動構築されたモバイル用ネットワーク) <a href=\"https://arxiv.org/abs/1807.11626\">(Mobile Neural Archtecture Search Network)\n",
    "    \n",
    "初回実行時には，datasetsと同様に自動的にダウンロードが行われます．（ですので，Proxyが必要な環境では，事前に環境変数を設定する必要があります．)\n",
    "\n",
    "他にも，object detection, semantic segmentation, video classification のモデルが用意されています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import time\n",
    "import os\n",
    "\n",
    "# proxyの設定．\n",
    "# keras.datasetsでは，datasetを直接ダウンロードするので，学内マシンからは通常必要．\n",
    "os.environ[\"http_proxy\"] = \"http://proxy.uec.ac.jp:8080/\"\n",
    "os.environ[\"https_proxy\"] = \"http://proxy.uec.ac.jp:8080/\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"      # \"0\":GPU0, \"1\":GPU1, \"0,1\":GPUを2つとも使用\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet50 による1000種類分類\n",
    "\n",
    "まずは，ResNet50 のpretrained modelを使って，1000種類認識をしてみましょう．\n",
    "以下のコードだけで実行できます．学習済モデルも自動的にダウンロードするので簡単です．\n",
    "なお，初回実行時は，モデルのダウンロードを行うので，結果が出るまで少し時間が掛かります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet50 による 1000種類分類\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合計 100100\r\n",
      "-rw------- 1 n2010495 aix-student 102502400  6月 23  2022 resnet50-19c8e357.pth\r\n"
     ]
    }
   ],
   "source": [
    "resnet50 = models.resnet50(pretrained=True,progress=True)\n",
    "softmax=nn.Softmax(dim=1)\n",
    "# pretrained=True とすると，学習済みポラメータも読み込まれる．\n",
    "# ~/.cache/torch/checkpoints/ の下に読み込まれます．\n",
    "# ls でダウンロードされていることを確認してみます．\n",
    "! ls -l ~/.cache/torch/checkpoints/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorchのpre-trained modelは，<a href=\"https://discuss.pytorch.org/t/whats-the-range-of-the-input-value-desired-to-use-pretrained-resnet152-and-vgg19/1683\">ここ</a> にかかれているように，画像は [0, 1] で表現されるように変換したものを，さらに，平均[0.485, 0.456, 0.406], 分散[0.229, 0.224, 0.225] となるように変換して学習されています．\n",
    "ですので，[0, 255]で読み込んだ画像を\n",
    "```python\n",
    "mean=np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "std=np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "img=(img/255.0-mean)/std\n",
    "```\n",
    "で，変換してから，学習済モデルに渡してやる必要があります．\n",
    "Pretrained model利用時に，これを行わないと，無意味な認識結果が出力されますので，十分に注意して下さい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img=np.array(Image.open('lion.jpeg').resize((224,224)), dtype=np.float32)\n",
    "mean=np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "std=np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "img=(img/255.0-mean)/std\n",
    "img=img.transpose(2,0,1)  # HWC -> CHW\n",
    "img=img[np.newaxis,...]  # adding batch axis\n",
    "img=torch.from_numpy(img)\n",
    "\n",
    "resnet50.eval() \n",
    "# batch_normalization を eval modelで計算するために，model.eval()で\n",
    "# eval modeを設定する．(学習時の平均分散を利用．)　\n",
    "# train modeだと，batch内の平均分散が使われるが，\n",
    "# この場合 batch_size=1 でbatch内平均分散が計算され，正しくない結果になる．\n",
    "# batch normalization を使ったモデルで認識する場合は，evel modeへの\n",
    "# 切り替えは必須なので，注意すること．\n",
    "\n",
    "with torch.no_grad(): # 勾配計算はしないので，no_grad modeで計算\n",
    "    out=softmax(resnet50(img)).numpy()[0]\n",
    "    # numpy()で，Tensor形式から numpy形式に変換\n",
    "    # batch_size=1 で1枚だけ認識したので，1枚目の結果だけをoutに入れるために[0]がついている\n",
    "\n",
    "top5   =np.sort(out)[:-6:-1]      # 昇順にソートされるので，最後の5つが top5\n",
    "top5idx=np.argsort(out)[:-6:-1]   # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.99926358 n02129165 lion, king of beasts, Panthera leo\n",
      "[2] 0.00011419 n02125311 cougar, puma, catamount, mountain lion, painter, panther, Felis concolor\n",
      "[3] 0.00009945 n02117135 hyena, hyaena\n",
      "[4] 0.00006722 n02397096 warthog\n",
      "[5] 0.00005424 n02422106 hartebeest\n"
     ]
    }
   ],
   "source": [
    "# 認識結果の top-5 の結果の表示\n",
    "SYNSET_FILE='synset_words.txt'  # ImageNet1000 種類のカテゴリ名が書かれたファイル．\n",
    "synset=open(SYNSET_FILE).read().split('\\n')\n",
    "for i in range(5):\n",
    "    print(\"[%d] %.8f %s\" % (i+1,top5[i],synset[top5idx[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に TorchVision を使った方法です．こちらの方が簡単です．\n",
    "\n",
    "TorchVision を使う場合は，以下の様にnormalizeします．\n",
    "```python\n",
    "normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.99930012 n02129165 lion, king of beasts, Panthera leo\n",
      "[2] 0.00013924 n02125311 cougar, puma, catamount, mountain lion, painter, panther, Felis concolor\n",
      "[3] 0.00006286 n02117135 hyena, hyaena\n",
      "[4] 0.00005809 n02422106 hartebeest\n",
      "[5] 0.00005807 n02130308 cheetah, chetah, Acinonyx jubatus\n"
     ]
    }
   ],
   "source": [
    "# 画像の変換は, TorchVisionを使うと簡単にできます．\n",
    "import torchvision.transforms as transforms\n",
    "image_size = (224, 224) \n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "image_transform = transforms.Compose([transforms.Resize(image_size),transforms.ToTensor(),normalize])\n",
    "img = Image.open('lion.jpeg')\n",
    "img = image_transform(img)\n",
    "img = img.unsqueeze(0)\n",
    "resnet50.eval() \n",
    "with torch.no_grad(): \n",
    "    out=softmax(resnet50(img)).numpy()[0]\n",
    "top5   =np.sort(out)[:-6:-1]   \n",
    "top5idx=np.argsort(out)[:-6:-1] \n",
    "for i in range(5):\n",
    "    print(\"[%d] %.8f %s\" % (i+1,top5[i],synset[top5idx[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet50 のネットワークを表示してみます．各res blockにskip connectionが入っていること(addのconnected to をたどってみましょう．)と，ネットワークの最後にGlobalAveragePooling (出力のfeature mapが 1x1 になっています), Flatten, Denseが入っていることを確認しましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ResNet50 の表示\n",
    "print(resnet50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 を使った特徴抽出\n",
    "\n",
    "VGG16 の fc2 から 4096次元特徴ベクトルを抽出してみましょう．\n",
    "初回実行時は，モデルのダウンロードを行うので，結果が出るまで少し時間が掛かります．\n",
    "\n",
    "4096次元ベクトルは，類似画像検索や，SVMを用いた画像分類の特徴量として利用できます．なお，画像分類は後述するfine-tuningでも可能で，一般にはそちらの方が高精度ですが，学習にはGPUが必要で時間が掛かるので，CPUのみで実行可能なSVMを使った画像分類の学習も場合によっては有用です．\n",
    "\n",
    "なお，データセットは，<a href=\"http://mm.cs.uec.ac.jp/animal.zip\">動物10種各100枚</a>を使ってください．\n",
    "Jupyter の terminal を開いて，Jupyterの作業ディレクトリに展開してください．\n",
    "```\n",
    "setenv http_proxy http://proxy.uec.ac.jp:8080/\n",
    "wget http://mm.cs.uec.ac.jp/animal.zip\n",
    "unzip animal.zip\n",
    "```\n",
    "としてください．(実行セルで， !(linux コマンド) としても実行できます．)\n",
    "\n",
    "なお，IEDの場合は /usr/local/class/object/animal, CEDの場合は/ced-home/staff/yanai/media/animal に同じデータセットがありますので，フルパス名を指定すれば，ダウンロード不要です．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合計 640568\r\n",
      "-rw------- 1 n2010495 aix-student 102502400  6月 23  2022 resnet50-19c8e357.pth\r\n",
      "-rw------- 1 n2010495 aix-student 553433881  2月 19 18:33 vgg16-397923af.pth\r\n"
     ]
    }
   ],
   "source": [
    "vgg16 = models.vgg16(pretrained=True,progress=True)\n",
    "softmax=nn.Softmax(dim=1)\n",
    "# pretrained=True とすると，学習済みポラメータも読み込まれる．\n",
    "# ~/.cache/torch/checkpoints/ に読み込まれます．VGG16は550MBもあるので，不要になったら消去しましょう．\n",
    "# ls でダウンロードされていることを確認してみます．\n",
    "! ls -l ~/.cache/torch/checkpoints/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.99754685 n02129165 lion, king of beasts, Panthera leo\n",
      "[2] 0.00033962 n02112137 chow, chow chow\n",
      "[3] 0.00028383 n02115913 dhole, Cuon alpinus\n",
      "[4] 0.00026045 n02106030 collie\n",
      "[5] 0.00013270 n02410509 bison\n"
     ]
    }
   ],
   "source": [
    "# 念のため，認識してみます．\n",
    "# 2位以下はResNetと若干異なっていますが，1位はあっているはずです．\n",
    "image_size = (224, 224) \n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "image_transform = transforms.Compose([transforms.Resize(image_size),transforms.ToTensor(),normalize])\n",
    "img = Image.open('lion.jpeg')\n",
    "img = image_transform(img)\n",
    "img = img.unsqueeze(0)\n",
    "vgg16.eval() \n",
    "with torch.no_grad():\n",
    "    out=softmax(vgg16(img)).numpy()[0]\n",
    "top5   =np.sort(out)[:-6:-1]   \n",
    "top5idx=np.argsort(out)[:-6:-1] \n",
    "for i in range(5):\n",
    "    print(\"[%d] %.8f %s\" % (i+1,top5[i],synset[top5idx[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (1): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (2): Flatten()\n",
      "  (3): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "  (4): ReLU(inplace=True)\n",
      "  (5): Dropout(p=0.5, inplace=False)\n",
      "  (6): Linear(in_features=4096, out_features=4096, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "    \n",
    "vgg16fc7 = torch.nn.Sequential(\n",
    "    vgg16.features,\n",
    "    vgg16.avgpool,\n",
    "    Flatten(),\n",
    "    *list(vgg16.classifier.children())[:-3]  # 最後の3つのlayer(relu,dropout,fc1000)を削除\n",
    ")\n",
    "# 表示してみます．fc7 (fc4096)が最終出力になっているはずです．\n",
    "print(vgg16fc7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4096])\n",
      "tensor([-0.4662, -2.8360, -3.2846,  0.6673, -1.7138, -4.5680,  1.4430, -1.3399,\n",
      "        -1.5306,  1.4850, -1.2114,  0.0782,  0.2708, -0.5227, -2.2403, -1.6590,\n",
      "        -1.8959, -1.2780, -1.6734, -0.1346, -3.0772, -2.9543, -2.2964,  0.0534,\n",
      "        -0.7952,  4.3078, -2.4280,  1.2097, -0.9758,  1.4676, -0.4221, -0.6301,\n",
      "        -1.3490, -0.1666, -2.0901, -1.1264, -1.8081, -2.0467, -2.2239, -3.1684,\n",
      "         0.2978, -1.4273,  0.6979, -2.1675, -1.2621,  0.3755,  4.6138,  0.1551,\n",
      "        -1.8192, -4.0548, -3.1900,  0.8393, -3.8521, -1.2662,  1.3195, -3.0540,\n",
      "        -0.9008, -2.7653, -0.1400, -2.2235, -1.8330, -0.7277,  1.0339, -1.2487,\n",
      "        -1.7718, -1.4427, -1.8328, -1.7777, -2.6309, -1.0341, -3.1651, -1.8286,\n",
      "         0.6546, -2.0965, -2.3978,  0.9357, -1.8812, -2.3502, -0.4309, -1.3155,\n",
      "        -2.1128, -2.8751, -3.5441, -1.5315, -2.1354,  0.0543, -3.0295, -2.0644,\n",
      "        -0.2059, -0.8311, -2.8010,  0.2827,  0.5275, -2.9884, -0.3949, -0.4119,\n",
      "        -0.6293, -3.3932, -2.0517,  0.7137])\n"
     ]
    }
   ],
   "source": [
    "vgg16fc7.eval()\n",
    "with torch.no_grad():\n",
    "    fc7=vgg16fc7(img)\n",
    "print(fc7.shape)     # shapeの表示\n",
    "print(fc7[0][0:100]) # fc7特徴量を最初の100次元分だけ表示してみます．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に学習画像の読み込みです．DataLoaderを使う方法もありますが，ここでは一気に2クラス分の200枚読み込んで，まとめてfc7特徴を抽出してしまいましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading 0th image\n",
      "reading 100th image\n",
      "(200, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "# imglist に cat100枚，dog100枚の合計200枚分のファイル名を用意します．\n",
    "# cat dog elephant fish horse lion penguin tiger whale wildcat があります．\n",
    "imglist=glob.glob('animal/lion/*.jpg')+glob.glob('animal/tiger/*.jpg')\n",
    "\n",
    "# 200枚画像をimgsに読み込みます．\n",
    "in_size=224\n",
    "imgs = np.empty((0,in_size,in_size,3), dtype=np.float32)\n",
    "\n",
    "for i,img_path in enumerate(imglist):\n",
    "    if i%100==0:\n",
    "        print(\"reading {}th image\".format(i))\n",
    "    x = np.array(Image.open(img_path).resize((in_size,in_size)), dtype=np.float32)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    imgs = np.vstack((imgs,x))\n",
    "    \n",
    "mean=np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "std=np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "imgs=(imgs/255.0-mean)/std\n",
    "imgs=imgs.transpose(0,3,1,2)  # HWC -> CHW\n",
    "img=torch.from_numpy(imgs)\n",
    "print(imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-1ac435acfc57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvgg16fc7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mfc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvgg16fc7\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m# gpuで処理した結果を cpuに戻して，numpy形式にします．\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# shapeの表示\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-4e0be295232d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m vgg16fc7 = torch.nn.Sequential(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "# 200枚処理するので，GPUを使います．\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "vgg16fc7 = vgg16fc7.to(device)\n",
    "\n",
    "vgg16fc7.eval()\n",
    "with torch.no_grad():\n",
    "    fc=vgg16fc7(img.to(device)).cpu().numpy()\n",
    "    # gpuで処理した結果を cpuに戻して，numpy形式にします．\n",
    "print(fc.shape)     # shapeの表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下では，抽出したfc特徴を，5-fold cross validationで学習・評価します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    " \n",
    "# 分類器のオブジェクトを生成．\n",
    "# Cはソフトマージンのハイパーパラメータです．\n",
    "model_svm = LinearSVC(C=1.0)\n",
    "num = len(fc)\n",
    "label=np.append(np.ones(num//2,dtype=np.float32),(-1)*np.ones(num//2,dtype=np.float32))\n",
    "acc=[]\n",
    "\n",
    "# 5 cross validation で分類精度評価\n",
    "for f in range(5):\n",
    "    # indexの作成．5で割ってf余る数がtestのindex, そうでない数がtrainのindex.\n",
    "    train=[n for n in range(num) if n%5!=f]\n",
    "    test =[n for n in range(num) if n%5==f]\n",
    "\n",
    "# トレーニングデータで学習する\n",
    "# label は 0,1のベクトルになります．\n",
    "    model_svm.fit(fc[train], label[train])\n",
    " \n",
    "# テストデータの分類をする\n",
    "#label_predict = model.predict(fc[test])  #0/1分類\n",
    "#predict_score = model.decision_function(fc[test]) #超平面からの符号つき距離\n",
    "    acc.append(model_svm.score(fc[test],label[test])) # 分類＋accuracyの計算\n",
    "    \n",
    "    # print('accuracy={:.5%}'.format(model_svm.score(fc[test],label[test])))\n",
    "print('accuracy={:.5%}'.format(np.average(acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 のファインチューニングによる小規模データセットの学習\n",
    "\n",
    "VGG16の学習済畳み込み層を使って，小規模画像データセットの学習を行ってみましょう．\n",
    "\n",
    "データセットは同じく動物データを使いますが，ここでは10クラスのマルチクラス分類を行います．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今度は，ImageLoaderを使ってみましょう．\n",
    "\n",
    "クラスごとにサブディレクトリが別れている場合は，<a href=\"https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder\">torchvision.datasets.ImageFolder</a>を使って，簡単に dataset objectを生成できます．ラベルデータも同時に作成してくれるので，手間が省けます．\n",
    "\n",
    "次に，<a href=\"https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset\">torch.utils.data.Subset</a>を使って，train/testにdatasetを分割します．\n",
    "\n",
    "あとは，MNISTの時と同じで，train/testの<a href=\"https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\">DataLoader</a>を生成します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# まず，image_transform を定義します．ImageFolderで指定するので，先に\n",
    "# 定義しておく必要があります．\n",
    "\n",
    "# リサイズ，CHW変換，正規化 の標準的な image_transform\n",
    "image_size = (224, 224) \n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "image_transform = transforms.Compose([transforms.Resize(image_size),transforms.ToTensor(),normalize])\n",
    "\n",
    "# リサイズ，CHW変換，正規化 に加えて，+-20度のランダム回転，\n",
    "# 0.8-1.2倍のランダム縦横比変換＋0.7-1.0倍のランダムクロップ，ランダム左右反転，のデータ拡張ありの image_transform\n",
    "image_transform_aug = transforms.Compose([transforms.RandomRotation(20),\n",
    "                                          transforms.RandomResizedCrop(image_size, scale=(0.7, 1.0), ratio=(0.8, 1.2)),\n",
    "                                          transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize\n",
    "                                        ])\n",
    "\n",
    "# ImageFolfer では，クラス毎にディレクトリがあって，そのクラスの画像が入っていることを想定しています．\n",
    "# animal/lion/*.jpg, animal/dog/*.jpg, animal/cat/*.jpg  ....のような感じです．\n",
    "#   animal datasetは，まさにそのようになっています．\n",
    "dataset=torchvision.datasets.ImageFolder(root=\"./animal\", transform=image_transform) \n",
    "\n",
    "# データ拡張ありの場合は，以下のを用いる． image_transform -> image_transform_aug\n",
    "augmentation=False # データ拡張しない場合は False，拡張する場合は True にする．\n",
    "\n",
    "if augmentation:\n",
    "    # データ拡張あり (学習データのみ)\n",
    "    dataset2=torchvision.datasets.ImageFolder(root=\"./animal\", transform=image_transform_aug) \n",
    "else:\n",
    "    # データ拡張なし\n",
    "    dataset2=dataset\n",
    "    \n",
    "num = len(dataset) # animal datasetの場合，1000\n",
    "\n",
    "# indexの作成．5で割り切れない数がtestのindex, 5の倍数がtrainのindex.\n",
    "# つまり，train:text=4:1=80%:20% とする．\n",
    "train_idx=[n for n in range(num) if n%5!=0]\n",
    "test_idx =[n for n in range(num) if n%5==0]\n",
    "\n",
    "trainset  = torch.utils.data.dataset.Subset(dataset2, train_idx)\n",
    "testset   = torch.utils.data.dataset.Subset(dataset, test_idx)\n",
    "\n",
    "batch_size=64\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=10)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次は，モデル定義です．\n",
    "\n",
    "VGG16を畳み込み層だけ読み込んだモデルvgg16と，その出力を新しく追加しする全結合2層のtop_modelを用意します．\n",
    "さらに，その2つのモデルをModel によって結合します．vgg16の出力をtop_modelの入力として，それを出力とする，vgg_modelを\n",
    "定義します．\n",
    "\n",
    "結果的に，元々のVGG16の全結合層だけを新しく入れ替えて，再学習することになります．vgg_modelの元々の部分は学習済のパラメータが設定されていて，一方，新しい部分はランダム値で初期化されます．\n",
    "\n",
    "このvgg_modelを学習します．ただし，vgg_modelの前半のレイヤーは taininable = False として，学習しないようにします．\n",
    "パラメータの更新が，追加した全結合と，vggの最後の畳み込み層のみになるので，学習時間が大幅に節約できます．\n",
    "\n",
    "このように fine-tuningでは，出力に近い層だけを学習するのが一般的です．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg16 のconvの学習済パラメータはfinetuneしないように凍結します．\n",
    "for param in vgg16.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# finetune用の model の定義．最後のlayerをcutして，nn.Linear(4096,10) を追加\n",
    "model = torch.nn.Sequential(\n",
    "    vgg16.features,\n",
    "    vgg16.avgpool,\n",
    "    Flatten(),\n",
    "    *list(vgg16.classifier.children())[:-1],  # 最後のlayer(fc1000)を削除\n",
    "    nn.Linear(4096,10)\n",
    ")\n",
    "# 表示してみます．10class分のfc出力が最終出力になっているはずです．\n",
    "print(model)\n",
    "\n",
    "# GPUに転送します．\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "課題3と同様に，学習中の経過のグラフ表示用にcallback用のShowGraphクラスを用意します．（ここは中身の理解不要．）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import time\n",
    "# callback のクラス ShowGraphを定義\n",
    "class ShowGraph:\n",
    "    def __init__(self,max_epoch):\n",
    "        # 表示エリアの設定\n",
    "        self.fig=plt.figure(figsize=(8,4))\n",
    "        self.fig1 = self.fig.add_subplot(121)\n",
    "        self.fig1.axis([0, max_epoch, 0.5, 1.0])\n",
    "        self.fig1.set_title('accuracy')\n",
    "        self.fig1.set_ylabel('accuracy')\n",
    "        self.fig1.set_xlabel('epoch')\n",
    "        self.fig2 = self.fig.add_subplot(122)\n",
    "        self.fig2.axis([0, max_epoch, 0, 5])\n",
    "        self.fig2.set_title('loss')\n",
    "        self.fig2.set_ylabel('loss')\n",
    "        self.fig2.set_xlabel('epoch')\n",
    "        self.max_epoch=max_epoch\n",
    "        self.start=time.time()\n",
    "    \n",
    "    # 学習の最初に呼び出される\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses=[]\n",
    "        self.losses_val=[]\n",
    "        self.acc=[]\n",
    "        self.acc_val=[]\n",
    "        self.n_epoch=[]\n",
    "        self.n_epoch_v=[]\n",
    "    \n",
    "    # 各epochの最後に呼び出される\n",
    "    def on_epoch_end(self, epoch, loss, acc, vloss, vacc):\n",
    "        self.n_epoch.append(epoch)\n",
    "        self.n_epoch_v.append(epoch)\n",
    "        self.acc.append(acc)\n",
    "        self.acc_val.append(vacc)     \n",
    "        self.losses.append(loss)\n",
    "        self.losses_val.append(vloss)    \n",
    "        self.test_acc=vacc\n",
    "    \n",
    "        display.clear_output(wait = True)\n",
    "        self.fig1.plot(self.n_epoch,self.acc,\"b\")\n",
    "        self.fig1.plot(self.n_epoch_v,self.acc_val,\"r\")\n",
    "        self.fig1.legend(['train', 'test'], loc='upper left')\n",
    "        self.fig2.plot(self.n_epoch,self.losses,\"b\")\n",
    "        self.fig2.plot(self.n_epoch_v,self.losses_val,\"r\")\n",
    "        self.fig2.legend(['train', 'test'], loc='upper right')\n",
    "        display.display(self.fig)\n",
    "        \n",
    "    def on_epoch_train(self, epoch, loss, acc): # validationを評価しないepochの表示用\n",
    "        self.n_epoch.append(epoch)\n",
    "        self.acc.append(acc)\n",
    "        self.losses.append(loss)\n",
    "    \n",
    "        display.clear_output(wait = True)\n",
    "        self.fig1.plot(self.n_epoch,self.acc,\"b\")\n",
    "#        self.fig1.legend(['train', 'test'], loc='upper left')\n",
    "        self.fig2.plot(self.n_epoch,self.losses,\"b\")\n",
    "#        self.fig2.legend(['train', 'test'], loc='upper right')\n",
    "        display.display(self.fig)\n",
    "              \n",
    "    # デストラクタ(オブジェクトが消滅時に実行される)  \n",
    "    # グラフが２つ表示されるのを防止．さらに最終val acc値の表示．\n",
    "    def __del__(self):\n",
    "        display.clear_output(wait = True)\n",
    "        print(\"val_acc: \",self.test_acc) \n",
    "        print('Time: ',time.time()-self.start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習を行う．課題3の MNIST とほぼ一緒．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch=25\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.1, min_lr=0.00001)\n",
    "#optimizer = optim.Adam(model.parameters())\n",
    "show_graph=ShowGraph(num_epoch)\n",
    "show_graph.on_train_begin();\n",
    "\n",
    "def train():\n",
    "    loss=0\n",
    "    total=0\n",
    "    total0=0\n",
    "    correct=0\n",
    "    model.train()\n",
    "    for i, (inputs, labels) in enumerate(trainloader, 0):\n",
    "        outputs = model(inputs.to(device))\n",
    "        labels=labels.to(device)\n",
    "        loss0= loss_fn(outputs, labels)\n",
    "        loss+= loss0.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        total0+=1\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        optimizer.zero_grad()\n",
    "        loss0.backward()\n",
    "        optimizer.step()\n",
    "    loss=loss/total0\n",
    "    acc=correct/total \n",
    "    return loss, acc\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "         vloss=0\n",
    "         total2=0\n",
    "         total20=0\n",
    "         correct2=0\n",
    "         for (inputs, labels) in testloader:\n",
    "             outputs = model(inputs.to(device))\n",
    "             labels=labels.to(device)\n",
    "             vloss += loss_fn(outputs, labels).item()\n",
    "             _, predicted = torch.max(outputs.data, 1)\n",
    "             total2 += labels.size(0)\n",
    "             total20+=1\n",
    "             correct2 += (predicted == labels).sum().item()\n",
    "         vloss=vloss/total20\n",
    "         vacc=correct2/total2\n",
    "    return vloss, vacc   \n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    l, a = train()   \n",
    "    lv, av = validate()\n",
    "    scheduler.step(lv) # val_lossが下がらなければlrを下げる\n",
    "    show_graph.on_epoch_end(epoch,l,a,lv,av)\n",
    "            \n",
    "del show_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 課題4\n",
    "\n",
    "以下の小問の(1)-(2)を解答すること．(3)は任意（できるだけやってみる方が望ましい．）．\n",
    "\n",
    "1. ResNet50, DenseNet, MobileNetV2で，3枚以上の画像について，それぞれ1000種類認識を行うこと．\n",
    "1. 上記のデータセットで，VGG16をfine-tuningして，画像分類を行うこと．データ拡張しない場合と，する場合を比較せよ．\n",
    "1. (2)と同様に, ResNet50, DenseNet など，別のネットワークで fine-tuningして，学習時間と精度を比較せよ．\n",
    "\n",
    "(2), (3)は以下のデータセットのどれかを利用すること．UEC-Food20 のみ20種類で，あとは10種類です．すべて1クラス100枚ずつ入っています．  \n",
    "（自分で用意可能な人は，自分で用意したものを利用してもよい．各カテゴリ100枚10クラス以上用意せよ．）\n",
    "\n",
    " * UEC-Food20 http://mm.cs.uec.ac.jp/uecfood20.zip\n",
    " * UEC-Food10 http://mm.cs.uec.ac.jp/uecfood10.zip (上記の10種類版．メモリが足りない場合にどうぞ．)\n",
    " * FlickrMaterialDatabase(FMD) http://mm.cs.uec.ac.jp/material10.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.51764375 n02123159 tiger cat\n",
      "[2] 0.19660324 n02123045 tabby, tabby cat\n",
      "[3] 0.16050665 n02124075 Egyptian cat\n",
      "[4] 0.04876548 n02127052 lynx, catamount\n",
      "[5] 0.01638939 n02497673 Madagascar cat, ring-tailed lemur, Lemur catta\n",
      "[1] 0.89653540 n02119022 red fox, Vulpes vulpes\n",
      "[2] 0.07118310 n02120505 grey fox, gray fox, Urocyon cinereoargenteus\n",
      "[3] 0.01108047 n02115913 dhole, Cuon alpinus\n",
      "[4] 0.00747862 n02119789 kit fox, Vulpes macrotis\n",
      "[5] 0.00690760 n02114712 red wolf, maned wolf, Canis rufus, Canis niger\n",
      "[1] 0.94556606 n02086646 Blenheim spaniel\n",
      "[2] 0.03725046 n02102177 Welsh springer spaniel\n",
      "[3] 0.00396449 n02102318 cocker spaniel, English cocker spaniel, cocker\n",
      "[4] 0.00362045 n02101388 Brittany spaniel\n",
      "[5] 0.00326058 n02086910 papillon\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "SYNSET_FILE='synset_words.txt'\n",
    "synset=open(SYNSET_FILE).read().split('\\n')\n",
    "\n",
    "resnet50 = models.resnet50(pretrained=True,progress=True)\n",
    "softmax=nn.Softmax(dim=1)\n",
    "\n",
    "img=np.array(Image.open('img/cat.jpg').resize((224,224)), dtype=np.float32)\n",
    "mean=np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "std=np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "img=(img/255.0-mean)/std\n",
    "img=img.transpose(2,0,1)\n",
    "img=img[np.newaxis,...]\n",
    "img=torch.from_numpy(img)\n",
    "\n",
    "resnet50.eval() \n",
    "\n",
    "with torch.no_grad():\n",
    "    out=softmax(resnet50(img)).numpy()[0]\n",
    "\n",
    "top5   =np.sort(out)[:-6:-1]\n",
    "top5idx=np.argsort(out)[:-6:-1]\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"[%d] %.8f %s\" % (i+1,top5[i],synset[top5idx[i]]))\n",
    "\n",
    "img=np.array(Image.open('img/fox.jpg').resize((224,224)), dtype=np.float32)\n",
    "mean=np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "std=np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "img=(img/255.0-mean)/std\n",
    "img=img.transpose(2,0,1)\n",
    "img=img[np.newaxis,...]\n",
    "img=torch.from_numpy(img)\n",
    "\n",
    "resnet50.eval() \n",
    "\n",
    "with torch.no_grad():\n",
    "    out=softmax(resnet50(img)).numpy()[0]\n",
    "\n",
    "top5   =np.sort(out)[:-6:-1]\n",
    "top5idx=np.argsort(out)[:-6:-1]\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"[%d] %.8f %s\" % (i+1,top5[i],synset[top5idx[i]]))\n",
    "\n",
    "img=np.array(Image.open('img/puppy.jpg').resize((224,224)), dtype=np.float32)\n",
    "mean=np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "std=np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "img=(img/255.0-mean)/std\n",
    "img=img.transpose(2,0,1)\n",
    "img=img[np.newaxis,...]\n",
    "img=torch.from_numpy(img)\n",
    "\n",
    "resnet50.eval() \n",
    "\n",
    "with torch.no_grad():\n",
    "    out=softmax(resnet50(img)).numpy()[0]\n",
    "\n",
    "top5   =np.sort(out)[:-6:-1]\n",
    "top5idx=np.argsort(out)[:-6:-1]\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"[%d] %.8f %s\" % (i+1,top5[i],synset[top5idx[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.63724232 n02123159 tiger cat\n",
      "[2] 0.15119442 n02124075 Egyptian cat\n",
      "[3] 0.13918592 n02123045 tabby, tabby cat\n",
      "[4] 0.03449495 n02123394 Persian cat\n",
      "[5] 0.01468451 n02127052 lynx, catamount\n",
      "[1] 0.98049682 n02119022 red fox, Vulpes vulpes\n",
      "[2] 0.01055632 n02120505 grey fox, gray fox, Urocyon cinereoargenteus\n",
      "[3] 0.00351647 n02119789 kit fox, Vulpes macrotis\n",
      "[4] 0.00287191 n02114712 red wolf, maned wolf, Canis rufus, Canis niger\n",
      "[5] 0.00093375 n02114855 coyote, prairie wolf, brush wolf, Canis latrans\n",
      "[1] 0.99725646 n02086646 Blenheim spaniel\n",
      "[2] 0.00144644 n02101388 Brittany spaniel\n",
      "[3] 0.00033826 n02102177 Welsh springer spaniel\n",
      "[4] 0.00031561 n02086910 papillon\n",
      "[5] 0.00027605 n02085782 Japanese spaniel\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "SYNSET_FILE='synset_words.txt'\n",
    "synset=open(SYNSET_FILE).read().split('\\n')\n",
    "\n",
    "densenet201 = models.densenet201(pretrained=True,progress=True)\n",
    "softmax=nn.Softmax(dim=1)\n",
    "\n",
    "img=np.array(Image.open('img/cat.jpg').resize((224,224)), dtype=np.float32)\n",
    "mean=np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "std=np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "img=(img/255.0-mean)/std\n",
    "img=img.transpose(2,0,1)\n",
    "img=img[np.newaxis,...]\n",
    "img=torch.from_numpy(img)\n",
    "\n",
    "densenet201.eval() \n",
    "\n",
    "with torch.no_grad():\n",
    "    out=softmax(densenet201(img)).numpy()[0]\n",
    "\n",
    "top5   =np.sort(out)[:-6:-1]\n",
    "top5idx=np.argsort(out)[:-6:-1]\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"[%d] %.8f %s\" % (i+1,top5[i],synset[top5idx[i]]))\n",
    "\n",
    "img=np.array(Image.open('img/fox.jpg').resize((224,224)), dtype=np.float32)\n",
    "mean=np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "std=np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "img=(img/255.0-mean)/std\n",
    "img=img.transpose(2,0,1)\n",
    "img=img[np.newaxis,...]\n",
    "img=torch.from_numpy(img)\n",
    "\n",
    "densenet201.eval() \n",
    "\n",
    "with torch.no_grad():\n",
    "    out=softmax(densenet201(img)).numpy()[0]\n",
    "\n",
    "top5   =np.sort(out)[:-6:-1]\n",
    "top5idx=np.argsort(out)[:-6:-1]\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"[%d] %.8f %s\" % (i+1,top5[i],synset[top5idx[i]]))\n",
    "\n",
    "img=np.array(Image.open('img/puppy.jpg').resize((224,224)), dtype=np.float32)\n",
    "mean=np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "std=np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "img=(img/255.0-mean)/std\n",
    "img=img.transpose(2,0,1)\n",
    "img=img[np.newaxis,...]\n",
    "img=torch.from_numpy(img)\n",
    "\n",
    "densenet201.eval() \n",
    "\n",
    "with torch.no_grad():\n",
    "    out=softmax(densenet201(img)).numpy()[0]\n",
    "\n",
    "top5   =np.sort(out)[:-6:-1]\n",
    "top5idx=np.argsort(out)[:-6:-1]\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"[%d] %.8f %s\" % (i+1,top5[i],synset[top5idx[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchvision.models' has no attribute 'mobilenetv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-fccc96973537>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSYNSET_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdensenet201\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmobilenetv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMobileNetV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torchvision.models' has no attribute 'mobilenetv2'"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "SYNSET_FILE='synset_words.txt'\n",
    "synset=open(SYNSET_FILE).read().split('\\n')\n",
    "\n",
    "densenet201 = models.mobilenetv2.MobileNetV2(pretrained=True,progress=True)\n",
    "softmax=nn.Softmax(dim=1)\n",
    "\n",
    "img=np.array(Image.open('img/cat.jpg').resize((224,224)), dtype=np.float32)\n",
    "mean=np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "std=np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "img=(img/255.0-mean)/std\n",
    "img=img.transpose(2,0,1)\n",
    "img=img[np.newaxis,...]\n",
    "img=torch.from_numpy(img)\n",
    "\n",
    "densenet201.eval() \n",
    "\n",
    "with torch.no_grad():\n",
    "    out=softmax(densenet201(img)).numpy()[0]\n",
    "\n",
    "top5   =np.sort(out)[:-6:-1]\n",
    "top5idx=np.argsort(out)[:-6:-1]\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"[%d] %.8f %s\" % (i+1,top5[i],synset[top5idx[i]]))\n",
    "\n",
    "img=np.array(Image.open('img/fox.jpg').resize((224,224)), dtype=np.float32)\n",
    "mean=np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "std=np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "img=(img/255.0-mean)/std\n",
    "img=img.transpose(2,0,1)\n",
    "img=img[np.newaxis,...]\n",
    "img=torch.from_numpy(img)\n",
    "\n",
    "densenet201.eval() \n",
    "\n",
    "with torch.no_grad():\n",
    "    out=softmax(densenet201(img)).numpy()[0]\n",
    "\n",
    "top5   =np.sort(out)[:-6:-1]\n",
    "top5idx=np.argsort(out)[:-6:-1]\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"[%d] %.8f %s\" % (i+1,top5[i],synset[top5idx[i]]))\n",
    "\n",
    "img=np.array(Image.open('img/puppy.jpg').resize((224,224)), dtype=np.float32)\n",
    "mean=np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "std=np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "img=(img/255.0-mean)/std\n",
    "img=img.transpose(2,0,1)\n",
    "img=img[np.newaxis,...]\n",
    "img=torch.from_numpy(img)\n",
    "\n",
    "densenet201.eval() \n",
    "\n",
    "with torch.no_grad():\n",
    "    out=softmax(densenet201(img)).numpy()[0]\n",
    "\n",
    "top5   =np.sort(out)[:-6:-1]\n",
    "top5idx=np.argsort(out)[:-6:-1]\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"[%d] %.8f %s\" % (i+1,top5[i],synset[top5idx[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
